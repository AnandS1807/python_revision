{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47654c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\anand\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\anand\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anand\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\anand\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anand\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.13.2)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anand\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anand\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.1/28.0 MB 10.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.7/28.0 MB 11.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 7.1/28.0 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 9.4/28.0 MB 11.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.8/28.0 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 13.9/28.0 MB 11.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.3/28.0 MB 11.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.9/28.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 21.2/28.0 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 23.6/28.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.0/28.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 16.1.0\n",
      "    Uninstalling pyarrow-16.1.0:\n",
      "      Successfully uninstalled pyarrow-16.1.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "Successfully installed datasets-4.4.2 dill-0.4.0 multiprocess-0.70.18 pyarrow-22.0.0 xxhash-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "696f1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e7193",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "#dataset  = load_dataset(\"---\") , here the dataset isnt a DDADTAFRAME, it's this:\n",
    "\n",
    " DatasetDict({\n",
    "    train: Dataset(...),\n",
    "    test: Dataset(...)\n",
    " })\n",
    "\n",
    "So:\n",
    "\n",
    "dataset = a collection of splits\n",
    "\n",
    "Each split (train, test, etc.) is its own dataset\n",
    "\n",
    "the training split :  dataset['train']\n",
    "so, dataset['train']  returns a Hugging Face Dataset object, not a pandas DataFrame.\n",
    "dataset['train'] behaves like a list of dictionaries, \n",
    "each row is a dictionary like this : \n",
    "  {\n",
    "    'text': 'I love this!',\n",
    "    'sentiment': 'positive',\n",
    "    ...\n",
    "  }\n",
    "\n",
    "df = pd.DataFrame(dataset['train']) #heere we convert that splkit to a dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "478f0fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text', 'label', 'label_text'],\n",
      "        num_rows: 26732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text', 'label', 'label_text'],\n",
      "        num_rows: 3432\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0cfd491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  label  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going      1   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "2  088c60f138                          my boss is bullying me...      0   \n",
       "3  9642c003ef                     what interview! leave me alone      0   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "\n",
       "  label_text  \n",
       "0    neutral  \n",
       "1   negative  \n",
       "2   negative  \n",
       "3   negative  \n",
       "4   negative  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbb16d",
   "metadata": {},
   "source": [
    "## Step 3: Tokenizer\n",
    "Now that we already have our dataset, we need a tokenizer to prepare it to be parsed by our model.\n",
    "\n",
    "As LLMs work with tokens, we require a tokenizer to process the dataset. To process your dataset in one step, use the Datasets map method to apply a preprocessing function over the entire dataset.\n",
    "\n",
    "This is why the second step is to load a pre-trained Tokenizer and tokenize our dataset so it can be used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt-2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0450275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Loading the dataset to train our model\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8504a5b",
   "metadata": {},
   "source": [
    "## explaination \n",
    "1Ô∏è. What is a tokenizer? (Intuition first)\n",
    "\n",
    "LLMs do not understand text.They only understand numbers. So we need a way to convert:\n",
    "\"I love NLP\" into something like:\n",
    "[40, 1842, 318]\n",
    "That converter is called a tokenizer.\n",
    "\n",
    "### What is a token?\n",
    "A token is not always a word.\n",
    "Depending on the tokenizer, a token can be:\n",
    "A word: \"love\"\n",
    "A subword: \"lov\" + \"e\"\n",
    "A character: \"l\", \"o\", \"v\", \"e\"\n",
    "\n",
    "Or even punctuation and spaces\n",
    "Example (GPT-2 tokenizer):\n",
    "\n",
    "\"I love NLP\"\n",
    "‚Üí [\"I\", \"ƒ†love\", \"ƒ†NL\", \"P\"]\n",
    "‚Üí [40, 1842, 16906, 47]\n",
    "(The ƒ† means ‚Äúspace before this token‚Äù)\n",
    "\n",
    "### What does a tokenizer produce?\n",
    "Usually three things:\n",
    "  input_ids ‚Üí numerical representation of tokens\n",
    "  attention_mask ‚Üí which tokens are real vs padding\n",
    "  (sometimes) token_type_ids\n",
    "\n",
    "Example output:\n",
    "\n",
    "    {\n",
    "      \"input_ids\": [40, 1842, 318, 50256, 50256],\n",
    "      \"attention_mask\": [1, 1, 1, 0, 0]\n",
    "    }\n",
    "\n",
    "## 2Ô∏è. Why do we tokenize when fine-tuning an LLM?\n",
    "üî¥ Raw text cannot be used by neural networks\n",
    "\n",
    "Neural networks only operate on numbers.\n",
    "So this is impossible:\n",
    "model(\"I love NLP\")  ‚ùå\n",
    "This is required:\n",
    "model([40, 1842, 318])  ‚úÖ\n",
    "\n",
    "During fine-tuning, tokenization ensures:\n",
    "1. Text becomes model-readable\n",
    "Tokens ‚Üí embeddings ‚Üí transformer layers\n",
    "\n",
    "2. Consistency with pretraining\n",
    "You must use the same tokenizer the model was trained with.\n",
    "GPT-2 was trained with: \"GPT2Tokenizer\"\n",
    "Using a different tokenizer would:\n",
    "  Break learned embeddings\n",
    "  Destroy model performance\n",
    "\n",
    "##  3. Fixed input size (padding & truncation)\n",
    "Models expect uniform tensor sizes in batches.\n",
    "So we:\n",
    "  Pad shorter sentences\n",
    "  Truncate longer ones\n",
    "Example:\n",
    "Max length = 10\n",
    "\n",
    "\"I love NLP\"\n",
    "‚Üí [I, love, NLP, <PAD>, <PAD>, ...]\n",
    "\n",
    "## 4. Efficient batching on GPU\n",
    "\n",
    "Same length ‚Üí faster matrix operations ‚Üí efficient training\n",
    "\n",
    "## 3Ô∏è. Now let‚Äôs explaination of the code\n",
    "üîπ Import tokenizer\n",
    "\n",
    "  `from transformers import GPT2Tokenizer\n",
    "You import the tokenizer used by GPT-2.\n",
    "This tokenizer:\n",
    "    Uses Byte Pair Encoding (BPE)\n",
    "    Has a fixed vocabulary\n",
    "    Matches GPT-2‚Äôs embeddings\n",
    "\n",
    "üîπ Load dataset\n",
    "  dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "\n",
    "This loads a DatasetDict:\n",
    "    {\n",
    "      \"train\": Dataset,\n",
    "      \"test\": Dataset\n",
    "    }\n",
    "\n",
    "Each row contains:\n",
    "\n",
    "    {\n",
    "      \"text\": \"...\",\n",
    "      \"sentiment\": \"positive\"\n",
    "    }\n",
    "\n",
    "üîπ Load pretrained tokenizer\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "What happens here:\n",
    "  Downloads GPT-2 tokenizer config\n",
    "\n",
    "Loads:\n",
    "  Vocabulary\n",
    "  Merge rules\n",
    "  Special tokens\n",
    "Now the tokenizer knows how GPT-2 splits text.\n",
    "\n",
    "üîπ Set padding token\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "Why is this needed?\n",
    "GPT-2 does not have a pad token by default.\n",
    "But padding is required for batching.\n",
    "So we reuse:\n",
    "  <eos> (end-of-sequence)\n",
    "as padding.\n",
    "\n",
    "This is very common for GPT-style models.\n",
    "\n",
    "üîπ Define tokenization function\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "      return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "What is examples?\n",
    "Because of batched=True, examples looks like:\n",
    "\n",
    "    {\n",
    "      \"text\": [\n",
    "        \"I love this movie\",\n",
    "        \"This is bad\"\n",
    "      ]\n",
    "    }\n",
    "\n",
    "What does this line do?\n",
    "\n",
    "    tokenizer(\n",
    "      examples[\"text\"],\n",
    "      padding=\"max_length\",\n",
    "      truncation=True\n",
    "    )\n",
    "\n",
    "It:\n",
    "\n",
    "    Converts text ‚Üí tokens\n",
    "    Converts tokens ‚Üí IDs\n",
    "    Pads all sequences to max length\n",
    "    Truncates if too long\n",
    "\n",
    "Returns:\n",
    "\n",
    "    {\n",
    "      \"input_ids\": [...],\n",
    "      \"attention_mask\": [...]\n",
    "    }\n",
    "\n",
    "üîπ Apply tokenization to entire dataset\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "What happens internally?\n",
    "Hugging Face:\n",
    "\n",
    "    Iterates over train and test\n",
    "    Applies tokenize_function\n",
    "    Adds new columns\n",
    "\n",
    "Now each row looks like:\n",
    "\n",
    "      {\n",
    "        \"text\": \"I love this!\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"input_ids\": [...],\n",
    "        \"attention_mask\": [...]\n",
    "      }\n",
    "\n",
    "üîπ Why use .map()?\n",
    "\n",
    "Because it:\n",
    "Is fast\n",
    "Is memory-efficient\n",
    "Works directly on datasets\n",
    "Avoids pandas overhead\n",
    "\n",
    "This is preferred over manually looping.\n",
    "\n",
    "Final mental model\n",
    "  Raw text\n",
    "    ‚Üì\n",
    "  Tokenizer\n",
    "    ‚Üì\n",
    "  Token IDs + Attention Mask\n",
    "    ‚Üì\n",
    "  Model embeddings\n",
    "    ‚Üì\n",
    "  Transformer layers\n",
    "    ‚Üì\n",
    "  Fine-tuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaed6ed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
